{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: folder=X:\\5_Research\\Paul\\dsi_origins\\\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pycountry \n",
    "import pycountry_convert as pc\n",
    "pp = pprint.PrettyPrinter()\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon\n",
    "import regex as re\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "#from fa2 import ForceAtlas2\n",
    "\n",
    "import textmining\n",
    "  \n",
    "\n",
    "%env folder = X:\\5_Research\\Paul\\dsi_origins\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = %env folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df\n",
    "df_wildsi_country =pd.read_csv(path + R'processed_data\\wildsi_country.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize publication identifiers\n",
    "pmids = list(df_wildsi_country['PRIMARY_PMID'])\n",
    "pmcids = list(df_wildsi_country['PRIMARY_PMCID'])\n",
    "dois = list(df_wildsi_country['PRIMARY_DOI'])\n",
    "\n",
    "\n",
    "pmids_clean = []\n",
    "for pubid in pmids: \n",
    "    if pd.isna(pubid): \n",
    "        pmids_clean.append(np.NaN)\n",
    "    else: \n",
    "        if type(pubid) == str: \n",
    "            if ';' in pubid: \n",
    "                pmids_clean.append(float(pubid.split(';')[0])) #multiple pubmeds given, so only take first one\n",
    "            else: \n",
    "                pmids_clean.append(float(pubid))\n",
    "        elif type(pubid) == float or type(pubid) == int: \n",
    "                pmids_clean.append(float(pubid))\n",
    "        else: \n",
    "            print(pubid)\n",
    "            print(type(pubid))\n",
    "\n",
    "pmcids_clean = []\n",
    "for pmcid in pmcids: \n",
    "    if pd.notna(pmcid): \n",
    "        pmcids_clean.append(pmcid.lower())\n",
    "    else: \n",
    "        pmcids_clean.append(np.nan)\n",
    "\n",
    "dois_clean = []\n",
    "for pub in list(dois):\n",
    "    pub_ed = pub\n",
    "    if pd.isna(pub_ed): \n",
    "        dois_clean.append(np.nan)\n",
    "    else: \n",
    "        if 'DOI' in pub_ed:\n",
    "            pub_ed = pub_ed.replace('DOI: ', '').replace('DOI:', '')\n",
    "            dois_clean.append(pub_ed)\n",
    "        elif ';' in pub_ed:\n",
    "            pub_ed = pub_ed.split(';')[0]\n",
    "            dois_clean.append(pub_ed)\n",
    "        elif 'Online' in pub_ed: \n",
    "            pub_ed = pub_ed.replace('Online First', '')\n",
    "            dois_clean.append(pub_ed)\n",
    "        elif ' ' in pub_ed: \n",
    "            pub_ed = pub_ed.replace(' ','')\n",
    "            dois_clean.append(pub_ed)\n",
    "        else: \n",
    "            dois_clean.append(pub_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wildsi_country['PRIMARY_PMID'] =pmids_clean\n",
    "df_wildsi_country['PRIMARY_PMCID'] = pmcids_clean\n",
    "df_wildsi_country['PRIMARY_DOI'] =dois_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in list(df_wildsi_country['PRIMARY_PMID'][df_wildsi_country['PRIMARY_PMID'].notna()]): \n",
    "    if type(x) != float: \n",
    "        print(type(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select ids working pub ids \n",
    "df_clean = df_wildsi_country[['ACCESSION', 'PRIMARY_PMID', 'PRIMARY_PMCID', 'PRIMARY_DOI']].dropna(subset= ['PRIMARY_PMID', 'PRIMARY_PMCID', 'PRIMARY_DOI'], how='all', inplace=False).drop_duplicates().reset_index(drop = True)\n",
    "df_clean['id_select'] = df_clean[['PRIMARY_PMID', 'PRIMARY_PMCID', 'PRIMARY_DOI']].bfill(axis=1).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[['ACCESSION','id_select']].to_csv(path+R'processed_data/acc_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wildsi_country_idselect = pd.merge(df_wildsi_country, df_clean[['ACCESSION','id_select']], how = 'left')\n",
    "df_wildsi_country_idselect.to_csv(path+R'processed_data/wildsi_country_idselect.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export pubid lists for downloading from the lens\n",
    "publist_pmid = list(set(df_wildsi_country['PRIMARY_PMID'][df_wildsi_country['PRIMARY_PMID'].notna()]))\n",
    "pd.DataFrame(publist_pmid, columns = ['PRIMARY_PMID']).to_csv(path+R'processed_data/clean_pmids.csv')\n",
    "\n",
    "#only take pmcids if no pmids available\n",
    "publist_pmcid = list(set(df_wildsi_country['PRIMARY_PMCID'][(df_wildsi_country['PRIMARY_PMCID'].notna()) & (df_wildsi_country['PRIMARY_PMID'].isna())]))\n",
    "pd.DataFrame(publist_pmcid, columns = ['PRIMARY_PMCID']).to_csv(path+R'processed_data/clean_pmcids.csv')\n",
    "\n",
    "#if no other are available, take DOIs\n",
    "publist_doi = list(set(df_wildsi_country['PRIMARY_DOI'][(df_wildsi_country['PRIMARY_DOI'].notna()) & (df_wildsi_country['PRIMARY_PMID'].isna()) & (df_wildsi_country['PRIMARY_PMCID'].isna())]))\n",
    "pd.DataFrame(publist_doi, columns = ['PRIMARY_DOI']).to_csv(path+R'processed_data/clean_dois.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_wildsi_country_idselect = pd.read_csv(path+R'processed_data/wildsi_country_idselect.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean = pd.read_csv(path+R'processed_data/acc_id.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12040057"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nr of unique accession numbers with publication ID\n",
    "len(set(df_clean['ACCESSION']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101111"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nr of unique publication IDs\n",
    "len(set(df_clean['id_select']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81147"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(publist_pmid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17490"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(publist_pmcid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2516"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(publist_doi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
