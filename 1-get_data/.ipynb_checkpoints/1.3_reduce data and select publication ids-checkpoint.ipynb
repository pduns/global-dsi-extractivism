{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: folder=X:\\5_Research\\Paul\\dsi_origins\\\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pycountry \n",
    "import pycountry_convert as pc\n",
    "pp = pprint.PrettyPrinter()\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon\n",
    "import regex as re\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "#from fa2 import ForceAtlas2\n",
    "\n",
    "import textmining\n",
    "  \n",
    "\n",
    "%env folder = X:\\5_Research\\Paul\\dsi_origins\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = %env folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "35000000\n",
      "36000000\n"
     ]
    }
   ],
   "source": [
    "filename = r'raw_data\\2024-07-28_ena_sequences.csv.gz'\n",
    "chunksize = 100000  # Adjust this size based on your system's memory constraints\n",
    "\n",
    "# Initialize a Counter to keep track of organism counts\n",
    "accessions = []\n",
    "pmids = []\n",
    "dois = []\n",
    "pmcids = []\n",
    "countries = []\n",
    "taxids = []\n",
    "organisms = []\n",
    "\n",
    "# Read the CSV in chunks\n",
    "chunk_count = 0\n",
    "for chunk in pd.read_csv(path + filename, compression='gzip', chunksize=chunksize, low_memory=False):\n",
    "        if chunk_count in list(range(0,100000000, 1000000)):\n",
    "            print(chunk_count)\n",
    "        \n",
    "        access = list(chunk['ACCESSION'])\n",
    "        accessions.extend(access)\n",
    "        \n",
    "        pmid = list(chunk['PRIMARY_PMID'])\n",
    "        pmids.extend(pmid)  \n",
    "        \n",
    "        doi = list(chunk['PRIMARY_DOI'])\n",
    "        dois.extend(doi)  \n",
    "        \n",
    "        pmcid = list(chunk['PRIMARY_PMCID'])\n",
    "        pmcids.extend(pmcid)\n",
    "        \n",
    "        taxid = list(chunk['TAXID'])\n",
    "        taxids.extend(taxid)\n",
    "        \n",
    "        organism = list(chunk['ORGANISM'])\n",
    "        organisms.extend(organism) \n",
    "\n",
    "        country = list(chunk['COUNTRY'])\n",
    "        countries.extend(country)\n",
    "        \n",
    "    #if chunk_count == 2: \n",
    "    #    break\n",
    "    \n",
    "        chunk_count = chunk_count+chunksize\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df\n",
    "df_raw =pd.DataFrame(zip(accessions, pubmeds, dois, pmcids, taxids, organisms, countries), columns = ['ACCESSION', 'PRIMARY_PMID', 'PRIMARY_DOI', 'PRIMARY_PMCID', 'TAXID', 'ORGANISM','COUNTRY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize publication identifiers\n",
    "pubmeds_clean = []\n",
    "for pubid in list(pmids): \n",
    "    if pd.isna(pubid): \n",
    "        pubmeds_clean.append(np.NaN)\n",
    "    else: \n",
    "        if type(pubid) == str: \n",
    "            if ';' in pubid: \n",
    "                pubmeds_clean.append(float(pubid.split(';')[0])) #multiple pubmeds given, so only take first one\n",
    "            else: \n",
    "                pubmeds_clean.append(float(pubid))\n",
    "        elif type(pubid) == float or type(pubid) == int: \n",
    "                pubmeds_clean.append(float(pubid))\n",
    "        else: \n",
    "            print(pubid)\n",
    "            print(type(pubid))\n",
    "\n",
    "pmcids_clean = []\n",
    "for pmcid in pmcids: \n",
    "    if pd.notna(pmcid): \n",
    "        pmcids_clean.append(pmcid.lower())\n",
    "    else: \n",
    "        pmcids_clean.append(np.nan)\n",
    "\n",
    "dois_clean = []\n",
    "for pub in list(dois):\n",
    "    pub_ed = pub\n",
    "    if pd.isna(pub_ed): \n",
    "        dois_clean.append(np.nan)\n",
    "    else: \n",
    "        if 'DOI' in pub_ed:\n",
    "            pub_ed = pub_ed.replace('DOI: ', '').replace('DOI:', '')\n",
    "            dois_clean.append(pub_ed)\n",
    "        elif ';' in pub_ed:\n",
    "            pub_ed = pub_ed.split(';')[0]\n",
    "            dois_clean.append(pub_ed)\n",
    "        elif 'Online' in pub_ed: \n",
    "            pub_ed = pub_ed.replace('Online First', '')\n",
    "            dois_clean.append(pub_ed)\n",
    "        elif ' ' in pub_ed: \n",
    "            pub_ed = pub_ed.replace(' ','')\n",
    "            dois_clean.append(pub_ed)\n",
    "        else: \n",
    "            dois_clean.append(pub_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df\n",
    "df =pd.DataFrame(zip(accessions, pubmeds_clean, dois_clean, pmcids_clean, taxids, organisms, countries), columns = ['ACCESSION', 'PRIMARY_PMID', 'PRIMARY_DOI', 'PRIMARY_PMCID', 'TAXID', 'ORGANISM','COUNTRY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in list(df['PRIMARY_PMID'][df['PRIMARY_PMID'].notna()]): \n",
    "    if type(x) != float: \n",
    "        print(type(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRIMARY_PMID_int'] = df['PRIMARY_PMID'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select ids working pub ids \n",
    "df_clean = df[['ACCESSION', 'PRIMARY_PMID_int', 'PRIMARY_PMCID', 'PRIMARY_DOI']].dropna(subset= ['PRIMARY_PMID_int', 'PRIMARY_PMCID', 'PRIMARY_DOI'], how='all', inplace=False).drop_duplicates().reset_index(drop = True)\n",
    "df_clean['id_select'] = df_clean[['PRIMARY_PMID_int', 'PRIMARY_PMCID', 'PRIMARY_DOI']].bfill(axis=1).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[['ACCESSION','id_select']].to_csv(path+R'processed_data/acc_id_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wildsi_idselect = pd.merge(df, df_clean[['ACCESSION','id_select']], how = 'left')\n",
    "df_wildsi_idselect.to_csv(path+R'processed_data/wildsi_idselect.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
